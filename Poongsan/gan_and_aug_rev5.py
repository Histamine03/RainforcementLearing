# -*- coding: utf-8 -*-
"""gan_aug_function

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BY_MpSsL6IfSBQmJuAB4ZjTcDqF0Ksxh
"""

# Commented out IPython magic to ensure Python compatibility.

class Data_loader:
    def __init__(self, data, label, batch_size=50, shuffle= False, device='cuda'):
        self.data = data
        self.n = len(data)
        self.label = label
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.device = device
        self.index = np.arange(self.n, dtype=np.int32)
        if self.shuffle==True:
            self.index = np.random.permutation(self.n).astype(np.int32)
    
    def shuffle_ftn(self):
        self.index = np.random.permutation(self.n).astype(np.int32)
    
    def __len__(self):
        return np.ceil(self.n/self.batch_size).astype(np.int32)

    
    def __iter__(self):
        st_indx = np.arange(0,self.n,self.batch_size)
        if self.n % self.batch_size == 0:
            ed_indx = np.arange(self.batch_size-1,self.n,self.batch_size)
        else:                        
            ed_indx = np.append(np.arange(self.batch_size-1,self.n,self.batch_size),self.n-1)

        n_bc = len(st_indx)
        index = self.index
        for st, ed in zip(st_indx, ed_indx):
            yield self.data[index[st:(ed+1)]].to(device=self.device), self.label[index[st:(ed+1)]].to(device=self.device)




import numpy as np
import pandas as pd
import os
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import torchvision
from PIL import Image
import torchvision.transforms as TRF
import torchvision.transforms.functional as TRF_F
import torchvision.utils as vutils
import math
import matplotlib.pyplot as plt
import re
import random
import cv2
import matplotlib.animation as animation

def init_weights(layer):
    cl_name = layer.__class__.__name__
    if cl_name.find('Conv') != -1:
        nn.init.normal_(layer.weight.data, 0.0, 0.02)
    elif cl_name.find('BatchNorm') != -1:
        nn.init.normal_(layer.weight.data, 0.0, 0.02)
        nn.init.constant_(layer.bias.data, 0)


def geom_augmentation(tensor_data, tensor_label, **kwargs):
    arg_list = np.array(['vflip','hflip','rotate','resize','resize_center_crop',
               'add_noise', 'affine'])
    arg_name =  np.array(list(kwargs.keys()))
    #print(arg_name)
    if np.all(np.isin(arg_name,arg_list))==False:
        print('Given arguments have non-valid arguments')
        return None
    
    aug_data_list = []
    aug_label_list = []
    
    if 'vflip' in arg_name:
        if kwargs['vflip']==True:
            out = TRF_F.vflip(tensor_data)
            aug_data_list.append(out)
            aug_label_list.append(tensor_label)
    
    if 'hflip' in arg_name:
        if kwargs['hflip']==True:
            out = TRF_F.hflip(tensor_data)
            aug_data_list.append(out)
            aug_label_list.append(tensor_label)
    
    if 'rotate' in arg_name:
        angle = kwargs['rotate']
        for ang in angle:
            out = TRF_F.rotate(tensor_data, float(ang))
            aug_data_list.append(out)
            aug_label_list.append(tensor_label)
    
    if 'resize' in arg_name: #[..., H, W]
        out = TRF_F.resize(tensor_data, size=kwargs['resize'])
        aug_data_list.append(out)
        aug_label_list.append(tensor_label)
    
    if 'resize_center_crop' in arg_name:
        org_hw = list(tensor_data.shape[-2:])
        out_size = kwargs['resize_center_crop']
        out_crop = TRF_F.center_crop(tensor_data, output_size=out_size)
        out = TRF_F.resize(out_crop, size=org_hw)
        aug_data_list.append(out)
        aug_label_list.append(tensor_label)
        
    if 'add_noise' in arg_name:
        org_shape = tensor_data.shape
        if kwargs['add_noise']['dist']=='unif':
            min_u, max_u = kwargs['add_noise']['param']
            noise_mat = np.random.uniform(min_u, max_u, size=org_shape)
        
        if kwargs['add_noise']['dist']=='norm':
            mean, sd = kwargs['add_noise']['param']
            noise_mat = np.random.uniform(mean, sd, size=org_shape)
        
        out = tensor_data + torch.tensor(noise_mat)
        aug_data_list.append(out)
        aug_label_list.append(tensor_label)
    
    if 'affine' in arg_name:
        out = TRF_F.affine(tensor_data, **kwargs['affine'])
        aug_data_list.append(out)
        aug_label_list.append(tensor_label)
    
    return torch.cat(aug_data_list).type(torch.float32), torch.cat(aug_label_list).type(torch.int64)

class Generator(nn.Module):
    def __init__(self, input_dim, output_col, num_node=64):
        super(Generator, self).__init__()
        self.in_dim = input_dim
        self.out_c = output_col
        self.n_node = num_node
        
        self.convtr1 = nn.ConvTranspose2d(self.in_dim, self.n_node*16, 4, 1, 0, bias=False) # 4x4
        self.convtr2 = nn.ConvTranspose2d(self.n_node*16, self.n_node*8, 4, 2, 1, bias=False) # 8x8
        self.convtr3 = nn.ConvTranspose2d(self.n_node*8, self.n_node*4, 4, 2, 1, bias=False) # 16x16
        self.convtr4 = nn.ConvTranspose2d(self.n_node*4, self.n_node*4, 4, 2, 1, bias=False) # 32x32
        self.convtr5 = nn.ConvTranspose2d(self.n_node*4, self.out_c, 4, 2, 1, bias=False) # 64x64
        
        self.bn1 = nn.BatchNorm2d(self.n_node*16)
        self.bn2 = nn.BatchNorm2d(self.n_node*8)
        self.bn3 = nn.BatchNorm2d(self.n_node*4)
        self.bn4 = nn.BatchNorm2d(self.n_node*4)
        
        self.tanh = nn.Tanh()
        
    def forward(self, x):
        x = F.relu(self.bn1(self.convtr1(x)))
        x = F.relu(self.bn2(self.convtr2(x)))
        x = F.relu(self.bn3(self.convtr3(x)))
        x = F.relu(self.bn4(self.convtr4(x)))
        x = self.tanh(self.convtr5(x))
        
        return x

class Discriminator(nn.Module):
    def __init__(self, input_col, num_node=64):
        super(Discriminator, self).__init__()

        self.in_c = input_col
        self.n_node = num_node
        
        self.conv1 = nn.Conv2d(self.in_c,self.n_node,(4,4),stride=2, padding=1, bias=False)
        self.conv2 = nn.Conv2d(self.n_node,self.n_node*2,(4,4),stride=2, padding=1, bias=False)
        self.conv3 = nn.Conv2d(self.n_node*2,self.n_node*4,(4,4),stride=2, padding=1, bias=False)
        self.conv4 = nn.Conv2d(self.n_node*4,self.n_node*8,(4,4),stride=2, padding=1, bias=False)
        #self.conv5 = nn.Conv2d(self.n_node*8,1,(4,4),stride=1, padding=0, bias=False)
        
        self.bn1 = nn.BatchNorm2d(self.n_node)
        self.bn2 = nn.BatchNorm2d(self.n_node*2)
        self.bn3 = nn.BatchNorm2d(self.n_node*4)
        self.bn4 = nn.BatchNorm2d(self.n_node*8)

        self.fc1 = nn.Linear(4*4*8*self.n_node, 8*self.n_node)
        self.fc2 = nn.Linear(8*self.n_node, 1)                        
    
    def forward(self, x):
        x = F.leaky_relu(self.bn1(self.conv1(x)),negative_slope=0.2)
        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.2)
        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.2)
        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.2)

        x = x.view(-1 ,4*4*8*self.n_node)
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        #x = torch.clamp(x, min=1e-5, max=1.0-1e-5)
        x = x.view(-1)
        
        #x = self.conv5(x)
        #x = torch.sigmoid(x)
        #x = torch.clamp(x, min=1e-5, max=1.0-1e-5)
        #x = torch.flatten(x)#x.view(-1)
        #x = x.view(-1,8*8*8*self.n_node)
        #x = F.relu(self.fc1(x))
        #x = torch.sigmoid(self.fc2(x))
        #x = x.view(-1)
        
        return x


def train_gan_model(gan_tr_data, gan_tr_lbl, num_nz=200, num_node=32, out_col = 1, num_epochs=10, 
                    batch_size=48,lr_rate = 0.0002, rec_step=5, device='cuda'):
    img_list = []
    G_losses = []
    D_losses = []
    iters = 0

    #device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    netG = Generator(num_nz, out_col, num_node).to(device)
    netG.apply(init_weights)

    netD = Discriminator(out_col, num_node).to(device)
    netD.apply(init_weights)

    criterion = nn.BCELoss()

    fixed_noise = torch.randn(64, num_nz, 1, 1, device=device)
    # Establish convention for real and fake labels during training
    real_label = 1.
    fake_label = 0.
    # Setup Adam optimizers for both G and D
    lr = lr_rate
    beta1 = 0.5
    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))
    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))

    #optimizerD = optim.SGD(netD.parameters(), lr=lr, momentum=0.9)
    #optimizerG = optim.SGD(netG.parameters(), lr=lr, momentum=0.9)


    gan_tr_data_load = Data_loader(gan_tr_data, gan_tr_lbl, batch_size=batch_size, shuffle=True)
    #print(len(gan_tr_data_load))
    print("Starting Training Loop...")
    for epoch in range(num_epochs):

        gan_tr_data_load.shuffle_ftn()

        for i, (data, y) in enumerate(gan_tr_data_load):

            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
            netD.zero_grad()
            # Format batch
            #b_size = data.size(0)
            #b_size = data.size(0)           
            #label = torch.full((b_size,), real_label, dtype=torch.float, device=device)
            # Forward pass real batch through D
            output = netD(data)
            b_size = output.size(0)           
            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)
            # Calculate loss on all-real batch
            errD_real = criterion(output, label)
            # Calculate gradients for D in backward pass
            errD_real.backward()
            D_x = output.mean().item()

            ## Train with all-fake batch
            # Generate batch of latent vectors
            noise = torch.randn(b_size, num_nz, 1, 1, device=device)
            # Generate fake image batch with G
            fake = netG(noise)
            label.fill_(fake_label)
            # Classify all fake batch with D
            output = netD(fake.detach()).view(-1)
            # Calculate D's loss on the all-fake batch
            errD_fake = criterion(output, label)
            # Calculate the gradients for this batch, accumulated (summed) with previous gradients
            errD_fake.backward()
            D_G_z1 = output.mean().item()
            # Compute error of D as sum over the fake and the real batches
            errD = errD_real + errD_fake
            # Update D
            optimizerD.step()

            # (2) Update G network: minimize -log(D(G(z)))
            netG.zero_grad()
            label.fill_(real_label)  
            output = netD(fake).view(-1)
            errG = criterion(output, label)
            # Calculate gradients for G
            errG.backward()
            D_G_z2 = output.mean().item()
            # Update G
            optimizerG.step()

            with torch.no_grad():
                label.fill_(fake_label)  
                errG2 = criterion(output,label) + errD_real # -log(D(x)) - log(1-D(G(z)))
            # Output training stats
            if iters % rec_step == 0:
                print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'
                      % (epoch, num_epochs, i, len(gan_tr_data_load), errD.item(), errG2.item(), D_x, D_G_z1, D_G_z2))

            # Save Losses for plotting later
            G_losses.append(errG2.item())
            D_losses.append(errD.item())
            # Check how the generator is doing by saving G's output on fixed_noise
            if (iters % rec_step == 0) or (epoch == num_epochs-1): # and (i == len(gan_tr_data_load)-1)):
                with torch.no_grad():
                    fake = netG(fixed_noise).detach().cpu()
                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))
                

            iters += 1
            
    return netG, netD, G_losses, D_losses, fake, img_list

def geom_aug(tensor_data, tensor_label):
    # tensor_data, tensor_label을 입력받고 aug된 data만 return하는 함수.
    # vflip, hflip, center_crop, rotate, add_noise, affine 총 6가지의 종류의 기하적 증강 실시.
    
    tensor_temp_data = tensor_data.clone()
    tensor_temp_label = tensor_label.clone()

    vflip_data, vflip_label = geom_augmentation(tensor_temp_data, tensor_temp_label,
                                                    vflip=True)

    hflip_data, hflip_label = geom_augmentation(tensor_temp_data, tensor_temp_label,
                                                        hflip=True)

    c_crop_data, c_crop_label = geom_augmentation(tensor_temp_data, tensor_temp_label, 
                                                         resize_center_crop=(48,48))

    ang_vec = np.arange(-20,25,20)
    ang_vec = ang_vec[ang_vec!=0]
    rotate_data, rotate_label = geom_augmentation(tensor_temp_data, tensor_temp_label,
                                                           rotate=ang_vec)

    param_dict = {'dist':'unif', 'param':[0, 0.1]}
    add_noise_data, add_noise_label = geom_augmentation(tensor_temp_data,
                                                      tensor_temp_label, add_noise=param_dict)

    param_dict = {'angle':30, 'translate':[5, 10], 'scale':1.2,
                     'shear':(0,0), 'fill':0.0}
    affine_data, affine_label = geom_augmentation(tensor_temp_data, tensor_temp_label,
                                                           affine=param_dict)
    
    aug_dat = torch.cat([vflip_data, hflip_data, c_crop_data,
                                 rotate_data, add_noise_data, affine_data])
    aug_lbl = torch.cat([vflip_label, hflip_label, c_crop_label,
                                 rotate_label, add_noise_label, affine_label])

    aug_dat = aug_dat.type(torch.float32)
    aug_lbl = aug_lbl.type(torch.int64)
    
    return aug_dat, aug_lbl

